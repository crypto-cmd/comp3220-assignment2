{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igZ2JPTz0_36"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGcwxt9r04Q7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# If failed to import, run: pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xjys01Y_7TXF"
   },
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SggIckg7aYL"
   },
   "source": [
    "Read from **heart_train.csv** into a pandas data frame(call it df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjXzUn0g1RYw"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yPh5VoP73K5"
   },
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnMEzI9g75w9"
   },
   "source": [
    "Try viewing the first five rows of your data (Note. try the head function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "tp9ROjTJ7weo",
    "outputId": "c8c3e8a0-8e7f-4610-dab4-72b407b5e8ff"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM2uHJ6C8EU1"
   },
   "source": [
    "Let's visualize our data bit and see number of people that have heart disease vs those who dont. In this particular dataset more people have heart disease than those who don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "CoU-h-Xb8UfE",
    "outputId": "d28cef27-551d-415a-b6eb-2d0cef272d12"
   },
   "outputs": [],
   "source": [
    "df['HeartDisease'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iApnltiL8qey"
   },
   "source": [
    "## Data Cleaning/PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdSEPifX8tp8"
   },
   "source": [
    "Before we contiune let us do some preprocessing on our data. Preprocessing is the process a data scientist or ML engineer goes through to make sure the data is clean and ready for the model. One example is checking to see if there are any null values in any of the columns and replacing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AASigvIK8_HR",
    "outputId": "80de5760-92c7-49e5-b5fe-6e2a444a3b6c"
   },
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UNWKqkhAdTr"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5Bg8DLJAfxY"
   },
   "source": [
    "Now Time to do some feature engineering. Extract values from columns you can use as features(hint try to use numerical columns). Store it an variable called X. Note do not use PatientId and remember to use .values to convert it to numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AxJXbEmAcKT"
   },
   "outputs": [],
   "source": [
    "# List of categorical columns to convert\n",
    "categorical_cols = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "\n",
    "# Apply One-Hot Encoding using pd.get_dummies\n",
    "# This converts columns like 'Sex' (M/F) into 'Sex_F' and 'Sex_M' (0s and 1s)\n",
    "df_clean = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# Display the new columns to verify\n",
    "print(\"New columns after encoding:\")\n",
    "print(df_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqERQkPpDgXI",
    "outputId": "a08a494a-fbc0-4620-b6e0-9197a5b878c8"
   },
   "outputs": [],
   "source": [
    "# Extract features into variable X\n",
    "# We drop 'PatientId' (not a feature) and 'HeartDisease' (the target label)\n",
    "# .values converts the Pandas DataFrame into a NumPy array, which is required for training\n",
    "X = df_clean.drop(['PatientId', 'HeartDisease'], axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fc6sM5XDQJ2"
   },
   "source": [
    "Extract your labels in a variable called y (HeartDisease column). Do the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyCiDB3DDc1v"
   },
   "outputs": [],
   "source": [
    "# Extract the target variable into y\n",
    "y = df_clean['HeartDisease'].values\n",
    "\n",
    "# Verify the shapes to ensure extraction was successful\n",
    "print(f\"\\nShape of X: {X.shape}\") # Should be (rows, number_of_features)\n",
    "print(f\"Shape of y: {y.shape}\") # Should be (rows,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-X4_5orEJt2"
   },
   "source": [
    "# Data Normalization\n",
    "\n",
    "We are going to now normalize our data. This will scale our data which will make it easier to train our model and make it more likley for our model to converge on the correct solution. Use the StandardScaler from sklearn to achieve this. Scale only the X variable. Store the result back into X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iuode4ReEI67"
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "\n",
    "# Normalize X\n",
    "# fit_transform calculates the mean and std dev for each feature, \n",
    "# then subtracts the mean and divides by the std dev. // mean 0, std 1\n",
    "X = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFXi1O5iFR0w",
    "outputId": "4296a39c-e580-426c-82ac-556a560c4658"
   },
   "outputs": [],
   "source": [
    "# Verify the result (Optional)\n",
    "# The values should now be small, typically between -2 and 2.\n",
    "print(\"First 5 rows of normalized X:\")\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKfFwFKeFhrv"
   },
   "source": [
    "# Train/Test Split\n",
    "\n",
    "We are now going to split our data between train and test. It is important to do this because we want to reduce the chance of overfitting so we dont want to test on the same data we just trained on. We will use the **train_test_split** function to achieve this. This has already been imported for you. Store the result in variables *X_train, y_train, X_test, y_test*. Use a *80/20* split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bV-ijNJEGx3G"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "# test_size=0.2 means 20% of the data goes to testing, 80% to training\n",
    "# random_state=42 ensures the split is reproducible (same split every time you run it)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnqzWcQMH_dp"
   },
   "source": [
    "Let us view the shape of the train data. The first number represents how many rows, the second represents how many columns or features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXuNlzdSIA0c",
    "outputId": "b31356fc-0b4e-437b-d016-474cc788c371"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RtS3q-cIK1T"
   },
   "source": [
    "Let us do the same for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlOifNcBIOA2",
    "outputId": "9131bd9b-d41d-4540-8042-763e83d5fe80"
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-idz7Xt6InOu"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5uZZCDpIosM"
   },
   "source": [
    "Let us create a model and fit the model to the train dataset.Let us use the LogisticRegression model from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsZqG6n9I3JG"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1. Create the Logistic Regression model\n",
    "# max_iter=1000 is often needed to ensure the solver converges on the solution\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-e9JM0TkJcwp"
   },
   "source": [
    "Call the fit function for the classifier on *X_train* and *y_train*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L68Uy3a8I9YO"
   },
   "outputs": [],
   "source": [
    "# 2. Fit the model to the training data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roevptciJNHA"
   },
   "source": [
    "We are now going to test our model. Call the score function on the classifier and pass in X_test, and y_test. The score you get represents the accuracy of the model e.g| a score of 0.9 means the model is 90% accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmlZk9itJG_3"
   },
   "outputs": [],
   "source": [
    "# 3. Test the results and report accuracy\n",
    "# The .score() method predicts on X_test and compares to y_test automatically\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcEi_eDYKAMC"
   },
   "source": [
    "# Neural Network\n",
    "\n",
    "Now let's try the same with a neural network. We will create a small neural network with some hidden layers and an output layer. (Note you are free to design this yourself). The network should output one value (try using sigmoid activation for last layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJO_T3EqKRZ-"
   },
   "outputs": [],
   "source": [
    "# 1. Create the Neural Network\n",
    "from tensorflow.keras.layers import Dropout # Import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Takes 20 things in, outputs 1 thing\n",
    "model.add(Dense(units=36, activation='relu', input_dim=X_train.shape[1], ))\n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dropout(0.2))  \n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghwimO53OEOO"
   },
   "outputs": [],
   "source": [
    "weight_decay=0.004 # is a good starting point for small data\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=0.001, \n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# 3. The Loss (Label Smoothing)\n",
    "label_smoothing=0.05 # prevents the model from being \"overconfident\"\n",
    "loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05)\n",
    "    \n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8w3w2nKOak0"
   },
   "source": [
    "Train the model, call the fit function and pass in X_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhmvqaj2OHyS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 3. Fit the model to the training data\n",
    "# Stop if validation loss doesn't improve for 5 epochs\n",
    "callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=8, callbacks=[callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma7f3RjUOixF"
   },
   "source": [
    "Let us now test the model. Call the evaluate function and pass in X_test and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "0XDMRehqONyK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.8841 - loss: 0.4266\n",
      "Neural Network Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "# 4. Test the results\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Neural Network Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EutRHsS8PIBc"
   },
   "source": [
    "## Test\n",
    "\n",
    "You are now going to test your model on the hold out test set. There is a file called **heart_test.csv**. You will notice that this file does not have a HeartDisease column. You will have to use your model to make predicitions on the test data. You will then create a file called submission.csv which you will upload to kaggle to see your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSu0jqaCRapp"
   },
   "source": [
    "Read heart_test.csv into a data frame called test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDyY0NJtRoEm"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('heart_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNv6_9SyRz_b"
   },
   "source": [
    "Let us view the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "4bUpK9WpR3xP",
    "outputId": "acf8e0d0-5d96-4411-e761-1cd8f320adb6"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkIxpCTbR-rW"
   },
   "source": [
    "Lets us now extract the same features as we did aboove to test on. You can call it X_new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEYG7BEhSGOi"
   },
   "outputs": [],
   "source": [
    "test_df_clean = pd.get_dummies(test_df, columns=categorical_cols)\n",
    "\n",
    "X_new = test_df_clean.drop(['PatientId'], axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aT1AuLXIS3VQ"
   },
   "source": [
    "We now need to normalize the test data as well. Use the scaler that you created above called sc and call the transform function and pass in X_new. Store the result back into X_new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnDHldRBTMEk"
   },
   "outputs": [],
   "source": [
    "X_new = sc.transform(X_new)  # Use transform, not fit_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMp03t_FShk-"
   },
   "source": [
    "Call the predict function on X_new to get the predicitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "ufAVdLR_Sg1S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions_tensor = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm-0V_3qHI-w"
   },
   "source": [
    "The neural network will output probabilties. We must convert those probabilites to 1 or 0. A probability greater than or equal to 0.5 is seen as a 1.Uncomment and run the cell below if the model you chose as your final model is a neural net created using tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "NLBzjx-hSxfc"
   },
   "outputs": [],
   "source": [
    "predictions = [1 if p >= 0.5 else 0 for p in predictions_tensor.squeeze()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DBVU-06Hdyw"
   },
   "source": [
    "# Submission\n",
    "Create a data frame with two columns PatientId and HeartDiesase (Try the pd.DataFrame function). The PatientId column should have the same values as the PatientId column from the test_df dataframe from above and HeartDisease column should be the predicitions you just created. Create a csv file from this data frame (Try using the .to_csv funtion, however make sure to remove indexes so set to the index flag to false). This should created a csv file, this is what you submit to kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "SgTxtRx7H6lj"
   },
   "outputs": [],
   "source": [
    "# Create the Submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'PatientId': test_df['PatientId'],\n",
    "    'HeartDisease': predictions\n",
    "})\n",
    "# Save to CSV\n",
    "# index=False removes the row numbers, which Kaggle doesn't want\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
